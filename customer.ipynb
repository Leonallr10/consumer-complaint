{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thava\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Explanatory Data Analysis and Feature Engineering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\thava/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7848317, 18)\n",
      "Columns: ['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'Consumer complaint narrative', 'Company public response', 'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?', 'Submitted via', 'Date sent to company', 'Company response to consumer', 'Timely response?', 'Consumer disputed?', 'Complaint ID']\n",
      "\n",
      "Missing values in [Product, Consumer complaint narrative]:\n",
      "Product                               0\n",
      "Consumer complaint narrative    5332868\n",
      "dtype: int64\n",
      "After filtering, dataset shape: (2515449, 2)\n",
      "Text length feature added. Sample lengths:\n",
      "9     1823\n",
      "13    2807\n",
      "14    1137\n",
      "19    2559\n",
      "20    1935\n",
      "Name: text_length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Step 1: Explanatory Data Analysis and Feature Engineering\n",
    "# -----------------------------\n",
    "print(\"Step 1: Explanatory Data Analysis and Feature Engineering\")\n",
    "\n",
    "# Download required NLTK resources (only once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load data â€“ adjust the file path as needed\n",
    "df = pd.read_csv('complaints.csv', dtype={'Consumer disputed?': str}, low_memory=False)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Check missing values for key columns\n",
    "print(\"\\nMissing values in [Product, Consumer complaint narrative]:\")\n",
    "print(df[['Product', 'Consumer complaint narrative']].isnull().sum())\n",
    "\n",
    "# Filter relevant columns and remove rows with missing complaint narratives\n",
    "df = df[['Product', 'Consumer complaint narrative']].dropna(subset=['Consumer complaint narrative'])\n",
    "print(\"After filtering, dataset shape:\", df.shape)\n",
    "\n",
    "# Feature Engineering: Add text length feature\n",
    "df['text_length'] = df['Consumer complaint narrative'].apply(len)\n",
    "print(\"Text length feature added. Sample lengths:\")\n",
    "print(df['text_length'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define category mapping dictionary\n",
    "category_map = {\n",
    "    0: ['Credit reporting', 'Credit card', 'Checking or savings account'],\n",
    "    1: ['Debt collection'],\n",
    "    2: ['Payday loan', 'Student loan', 'Vehicle loan'],\n",
    "    3: ['Mortgage']\n",
    "}\n",
    "\n",
    "def map_category(product):\n",
    "    product = str(product).lower()\n",
    "    for cat, keywords in category_map.items():\n",
    "        for kw in keywords:\n",
    "            if kw.lower() in product:\n",
    "                return cat\n",
    "    return 0  # Default category if no keywords found\n",
    "\n",
    "# Create the target variable \"category\"\n",
    "df['category'] = df['Product'].apply(map_category)\n",
    "print(\"Category mapping completed. Category distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Visualizations for EDA\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df['category'].value_counts().plot(kind='bar')\n",
    "plt.title('Category Distribution')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['text_length'], bins=50)\n",
    "plt.title('Text Length Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 2: Text Pre-Processing\n",
    "# -----------------------------\n",
    "print(\"\\nStep 2: Text Pre-Processing\")\n",
    "\n",
    "# Set explicit NLTK data path (optional, if needed)\n",
    "nltk_data_path = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download additional NLTK resources if necessary\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "# (Other downloads already done above)\n",
    "\n",
    "# Check if cleaned text column exists; if not, run preprocessing\n",
    "if 'clean_text' not in df.columns:\n",
    "    print(\"Running text preprocessing...\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        try:\n",
    "            text = str(text).lower()\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and numbers\n",
    "            words = nltk.word_tokenize(text)\n",
    "            words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "            return ' '.join(words)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    tqdm.pandas(desc=\"Preprocessing Progress\")\n",
    "    df['clean_text'] = df['Consumer complaint narrative'].progress_apply(preprocess_text)\n",
    "    print(\"Text preprocessing completed. Here are some cleaned texts:\")\n",
    "    print(df['clean_text'].head())\n",
    "else:\n",
    "    print(\"Clean text column already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 3: Selection of Multi-Classification Model\n",
    "# -----------------------------\n",
    "print(\"\\nStep 3: Selection of Multi-Classification Model\")\n",
    "# For demonstration, we use three models: Naive Bayes, SVM, and Random Forest.\n",
    "print(\"Selected models: Naive Bayes, Linear SVM (with balanced class weight), and Random Forest (with balanced class weight).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 4: Comparison of Model Performance\n",
    "# -----------------------------\n",
    "print(\"\\nStep 4: Comparison of Model Performance\")\n",
    "# Vectorization using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = tfidf.fit_transform(df['clean_text'])\n",
    "# IMPORTANT: Ensure the target column exists!\n",
    "if 'category' not in df.columns:\n",
    "    raise KeyError(\"The 'category' column is missing. Please run the category mapping step first.\")\n",
    "y = df['category']\n",
    "\n",
    "# Split data (80/20) with stratification to handle class imbalance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "print(\"Data split into training and testing sets.\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': LinearSVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Train each model and compare their performance\n",
    "results = {}\n",
    "for name, model in tqdm(models.items(), desc=\"Training Models\"):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results[name] = {'accuracy': acc, 'report': report, 'model': model}\n",
    "    \n",
    "    print(f\"{name} Performance:\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Model Evaluation\n",
    "# -----------------------------\n",
    "print(\"\\nStep 5: Model Evaluation\")\n",
    "# For demonstration, we use the best model (assumed here as Random Forest based on accuracy)\n",
    "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"Selected best model: {best_model_name}\")\n",
    "\n",
    "# Generate and display confusion matrix for the best model\n",
    "y_best_pred = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_best_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(f\"Confusion Matrix for {best_model_name}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Step 6: Prediction\n",
    "# -----------------------------\n",
    "print(\"\\nStep 6: Prediction\")\n",
    "# Example: Classify a new complaint narrative\n",
    "sample_text = (\"I was unfairly charged on my credit card and my report was negatively affected. \"\n",
    "               \"I need assistance with credit repair.\")\n",
    "print(\"Original sample complaint narrative:\")\n",
    "print(sample_text)\n",
    "\n",
    "# Preprocess the sample text using the same preprocessing function\n",
    "sample_clean = preprocess_text(sample_text)\n",
    "sample_vector = tfidf.transform([sample_clean])\n",
    "predicted_category = best_model.predict(sample_vector)[0]\n",
    "\n",
    "print(\"Predicted category for the sample complaint:\")\n",
    "print(predicted_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
